\documentclass[8pt,a4paper,oneside,hidelinks,aspectratio=169,dvipsnames]{beamer}
\usepackage[outputdir=build]{minted}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{datetime}
\usepackage{lmodern}
\usepackage{graphicx}
\usepackage{csquotes}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{colortbl}
\usepackage{siunitx}
\usepackage{tabularx}
\usepackage{longtable}
\usepackage{amsmath}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{pifont}
\usepackage{xspace}
\usepackage{pgfplots}
\usepackage{tikz}
\usepackage{circuitikz}
\usepackage{tikz-timing}
\usepackage[export]{adjustbox}

\usetheme[progressbar=frametitle]{metropolis}
\setbeamerfont{block title}{size=\small}
\setbeamertemplate{section in toc}[sections numbered]
\captionsetup[figure]{font=tiny,labelsep=none}
\usetikzlibrary{automata, arrows.meta, shapes.geometric, calc, positioning, fit}

\newcommand{\grayline}{\arrayrulecolor{gray!40}\hline\arrayrulecolor{black}}
\newcommand{\cmark}{\ding{51}\xspace}%
\newcommand{\xmark}{\ding{55}\xspace}%
\newcommand{\code}[1]{\texttt{\detokenize{#1}}}
\newcommand{\codecpp}[1]{\mintinline[fontsize=\small]{C++}{#1}}

\title{Accelerating Halide on an FPGA by using CIRCT and Calyx as an intermediate step to go from a high-level and software-centric IRs down to RTL}
\date{May 15, 2023}
\author{Sergi Granell Escalfet}
\institute[Facultat d’Informàtica de Barcelona] {
  Master Degree in Innovation and Research in Informatics - High Performance Computing \\
  Facultat d’Informàtica de Barcelona \\
  Universitat Politècnica de Catalunya - BarcelonaTech
}

\pgfdeclareimage[height=0.6cm]{university-logo}{img/logo-upc-fib.png}
%\logo{\pgfputat{\pgfxy(-1.4,-1.0)}{\pgfbox[center,base]{\pgfuseimage{university-logo}}}}

\begin{document}

\maketitle

\begin{frame}
  \frametitle{Table of Contents}
  \tableofcontents
\end{frame}

\section{Introduction}

\begin{frame}{Image and array processing}
  \begin{itemize}
    \item Image processing and array processing play an essential role in modern life:
          \begin{itemize}
            \item Applying filters to the images that we upload to social media
            \item Running object detection algorithms on self-driving cars
          \end{itemize}
    \item $\uparrow$ \textbf{Sophistication} modern image processing pipelines, resolution image sensors, real-time video processing $\implies$ $\uparrow$ demand for highly \textbf{efficient} image processing pipeline implementations
    \item \textbf{Diversity} of targets: from a small device such as a smartphone, smartwatch or edge device to large data center and HPC systems
    \item Optimizing these algorithms can be \textbf{complex} and often results in \textbf{non-portable code}
          \begin{itemize}
            \item Hand-tuned C and assembly for a specific architecture
            \item Implementations optimized for an x86 multicore and a modern GPU have little resemblance
          \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{Domain Specific Languages (DSLs)}
  \begin{itemize}
    \item DSLs: programming languages specialized to a \textbf{particular application domain}
    \item \textbf{Abstraction}: they provide a higher level of abstraction tailored to the specific domain
          \begin{itemize}
            \item Making it easier for developers to express complex concepts and ideas in a concise and natural way
          \end{itemize}
    \item \textbf{Expressiveness}: by focusing on a specific domain, DSLs enable developers to express their intent more directly, resulting in more readable and maintainable code
    \item \textbf{Productivity}: they simplify the development process. Focus on solving domain-specific problems rather than low-level implementation details
    \item \textbf{Performance}: they can be optimized for the specific domain, potentially allowing more efficient execution and better performance
    \item For the image/array processing application domain $\implies$ \textbf{Halide}
  \end{itemize}
\end{frame}

\section{Halide}

\begin{frame}[fragile]{Halide}
  \begin{itemize}
    \item Main idea: \textbf{decouple} the \textcolor{blue}{\textbf{\textit{algorithm}}} definition (\textquote{what needs to be computed}) from its \textcolor{blue}{\textbf{\textit{schedule}}} (\textquote{how it should be computed})
    \item W/o changing algorithm, explore different optimizations strategies (loop nesting and loop fusion, tiling, recomputation and storage balancing, vectorization, parallelism, \ldots)
  \end{itemize}
  \begin{figure}[H]
    \centering
    \begin{minipage}{\textwidth}
      \centering
      \begin{subfigure}[H]{.4\textwidth}
        \inputminted[tabsize=2,frame=single,rulecolor=gray,fontsize=\fontsize{4.2}{3}]{cpp}{fig/halide_manual_opt.cpp}
        \caption*{Hand-optimized C\texttt{++}. $\times11$ faster than naive impl., 0.9 ms/megapixel.}
      \end{subfigure}
      $\implies$
      \begin{subfigure}[H]{.4\textwidth}
        \inputminted[escapeinside=||,tabsize=2,frame=single,rulecolor=gray,fontsize=\fontsize{4.2}{3}]{cpp}{fig/halide_blur_3x3.cpp}
        \caption*{0.9 ms/megapixel.}
      \end{subfigure}
    \end{minipage}
  \end{figure}
\end{frame}

\begin{frame}[fragile]{Scheduling trade-offs (1)}
  \fontsize{6pt}{7.2}\selectfont
  \begin{figure}[H]
    \begin{minipage}{0.325\textwidth}
      \centering
      Blur 3x3 filter algorithm
      \centering
      \inputminted[tabsize=2,frame=single,rulecolor=gray,fontsize=\fontsize{5}{5}]{cpp}{fig/blur_3x3_base.cpp}
    \end{minipage}
  \end{figure}
  %\vspace{-0.5cm}
  \begin{figure}[H]
    \makebox[\linewidth]{
      \begin{minipage}{1.2\textwidth}
        \centering
        \begin{subfigure}[H]{.275\textwidth}
          \centering
          Breadth-first strategy
          \inputminted[tabsize=2,frame=single,rulecolor=gray,fontsize=\fontsize{4.2}{3}]{cpp}{fig/blur_3x3_breadth_first.cpp}
          \centering
          \includegraphics[width=3cm]{img/halide-breadth-first.png}
          \begin{itemize}
            \item[\xmark\xmark] Producer-consumer locality
            \item[\cmark\cmark] Parallelization
            \item[\cmark\cmark] Recomputation
          \end{itemize}
        \end{subfigure}
        \begin{subfigure}[H]{.31\textwidth}
          \centering
          Total fusion/inline strategy
          \inputminted[tabsize=2,frame=single,rulecolor=gray,fontsize=\fontsize{4.2}{3}]{cpp}{fig/blur_3x3_total_fusion.cpp}
          \vspace{0.3cm}
          \centering
          \includegraphics[width=3cm]{img/halide-total-fusion.png}
          \begin{itemize}
            \item[\cmark\cmark] Producer-consumer locality
            \item[\cmark\cmark] Parallelization
            \item[\xmark\xmark] Recomputation
          \end{itemize}
        \end{subfigure}
        \begin{subfigure}[H]{.3475\textwidth}
          \centering
          Sliding window strategy
          \inputminted[tabsize=2,frame=single,rulecolor=gray,fontsize=\fontsize{4.2}{3}]{cpp}{fig/blur_3x3_sliding_window.cpp}
          \centering
          \includegraphics[width=3cm]{img/halide-sliding-window.png}
          \begin{itemize}
            \item[\cmark\cmark] Producer-consumer locality
            \item[\xmark\xmark] Parallelization
            \item[\cmark\cmark] Recomputation
          \end{itemize}
        \end{subfigure}
      \end{minipage}
    }
    \centering
    \unskip
    \vspace{0.3cm}
    \begin{tikzpicture}
      \coordinate (a) at (0cm,0cm);
      \coordinate (b) at (1.5cm,0);
      \coordinate (c) at (60:1.5cm);
      %
      \draw[color=black, fill=gray!40] (a) -- (b) -- (c) -- cycle;
      %
      \node[right = 0.1cm of b] {redundant work};
      \node[left = 0.1cm of a] {locality};
      \node[above = 0.1cm of c] {parallelism};
      \node[align=center] at (0.75cm, 0.4cm) {\tiny tradeoff\\\tiny space};
    \end{tikzpicture}
  \end{figure}
\end{frame}

\begin{frame}[fragile]{Scheduling trade-offs (2)}
  \fontsize{6pt}{7.2}\selectfont
  \vspace{0.25cm}
  \begin{figure}[H]
    \makebox[\linewidth]{
      \begin{minipage}{\textwidth}
        \centering
        \begin{subfigure}[H]{.45\textwidth}
          \centering
          Tiling strategy
          \inputminted[tabsize=2,frame=single,rulecolor=gray,fontsize=\fontsize{4.2}{3}]{cpp}{fig/blur_3x3_tiling.cpp}
          \centering
          \includegraphics[width=3cm]{img/halide-tiling.png}
          \begin{itemize}
            \item[\cmark] Producer-consumer locality
            \item[\cmark\cmark] Parallelization
            \item[\xmark] Recomputation
          \end{itemize}
        \end{subfigure}
        \begin{subfigure}[H]{.45\textwidth}
          \centering
          Sliding window within tiling strategy
          \inputminted[tabsize=2,frame=single,rulecolor=gray,fontsize=\fontsize{4.2}{3}]{cpp}{fig/blur_3x3_sliding_window_tiling.cpp}
          \vspace{0.4cm}
          \centering
          \includegraphics[width=3cm]{img/halide-sliding-window-tiling.png}
          \begin{itemize}
            \item[\cmark] Producer-consumer locality
            \item[\cmark] Parallelization
            \item[\cmark] Recomputation
          \end{itemize}
        \end{subfigure}
      \end{minipage}
    }
  \end{figure}
  \vspace{0.75cm}
  \metroset{block=fill}
  \begin{exampleblock}{Note}
    The best scheduling choice differs depending on each target architecture and the computational characteristics of the image pipeline stages.
  \end{exampleblock}
\end{frame}

\begin{frame}{Halide scheduling example}
  \fontsize{6pt}{7.2}\selectfont
  \begin{enumerate}
    \item Function's \textit{domain} is \textit{tiled} into $64\times64$-sized tiles
    \item Outer two loops of \textit{tiling} are \textit{fused} together into a single loop (\code{tile_index})
    \item \textit{Fused} loop is \textit{parallelized}
    \item Each \textit{tile} is \textit{tiled} again with $4\times2$-sized sub-tiles
    \item Sub-tile innermost $x$-loop (\code{x_vectors}) is \textit{vectorized} with the same factor as \code{x_vectors} ($4$): no iterations will be performed at this nesting level, the whole loop will be vectorized
    \item Sub-tile $y$-loop (\code{y_pairs}) is fully \textit{unrolled} with a factor matching the sub-tile vertical size ($2$), therefore eliminating the sub-tile inner loops in favor of \textit{unrolling} and \textit{vectorization}
  \end{enumerate}

  \begin{figure}[H]
    \begin{minipage}{0.6\textwidth}
      \centering
      \inputminted[tabsize=2,frame=single,rulecolor=gray,fontsize=\fontsize{5}{5}]{cpp}{fig/halide_scheduling_example.cpp}
    \end{minipage}
  \end{figure}
\end{frame}

\begin{frame}{Halide compilation flow}
  \fontsize{6pt}{7.2}\selectfont
  \begin{enumerate}
    \item \textbf{Lowering and loop synthesis}: given the \textit{schedule}, it generates the loop nests and allocations required to evaluate the pipeline, beginning from the output.
    \item \textbf{Bounds inference}: recursively back from the output and using interval analysis, for each function, it evaluates the bounds of the dimensions based on the bounds required by its caller and the indices it is called with.
    \item \textbf{Sliding window optimization and storage folding}: traverses the loop nests seeking opportunities for sliding window optimizations (when the results of a function are to be stored by a serial loop at a higher loop nesting level than its computation).
    \item \textbf{Flattening}: multi-dimensional loads, stores, and allocations are flattened into their linear single-dimensional equivalent.
    \item \textbf{Vectorization and unrolling}: converts loops that were scheduled as vectorized or unrolled into the corresponding loops. During vectorization, occurrences of a loop index are replaced with a special value $ramp(n)$ which represents the vector $[0, 1, \ldots, n-1]$.
    \item \textbf{Back-end code generation}: low-level optimizations are performed and machine code is emitted for the resulting pipeline. After running constant-folding and dead-code elimination passes, the Halide IR is ready to be lowered with a \code{CodeGen} backend. The primary backends use LLVM for code generation.
  \end{enumerate}

  \begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{img/halide-compiler-flow.png}
  \end{figure}

\end{frame}

\begin{frame}{Halide IR}
  \begin{itemize}
    \item Halide IR nodes have an explicit type described by \codecpp{enum IRNodeType}. Examples:
          \begin{itemize}
            \item \codecpp{IntImm} to create integer immediates
            \item \code{Add} to represent additions
            \item \codecpp{Store} and \code{Load} to perform memory accesses
          \end{itemize}
    \item \codecpp{struct IRNode}: \codecpp{IRNodeType} + reference count. Virtual method \codecpp{accept} to implement the \textbf{visitor pattern}.
          \begin{itemize}
            \item \codecpp{IRVisitor}: traverse the IR nodes and perform some action on it (like generating code), but without modifying them.
            \item \codecpp{IRMutator}: traverse the IR nodes to modify them.
          \end{itemize}
    \item Two kinds of IR nodes, analogously to C:
          \begin{itemize}
            \item \textbf{Expressions} (\codecpp{ExprNode}): represent some value and have some type (e.g. \codecpp{x + 3})
            \item \textbf{Statements} (\codecpp{StmtNode}): side-effecting pieces of code that do not represent a value (e.g. \codecpp{assert(x > 3)}, \codecpp{store}).
          \end{itemize}
    \item Type system: signed and unsigned ints, IEEE fp numbers, opaque pointers (like \codecpp{void *}) and \code{bfloat}
  \end{itemize}
  \vspace*{0.1cm}
  \begin{figure}[H]
    \makebox[\linewidth]{
      \begin{minipage}[H]{\textwidth}
        \centering
        \begin{subfigure}[t]{.45\textwidth}
          \centering
          \codecpp{IntImm} node definition
          \inputminted[tabsize=2,frame=single,rulecolor=gray,fontsize=\fontsize{4.5}{4}]{cpp}{fig/halide_IntImm_node.cpp}
        \end{subfigure}
        \begin{subfigure}[t]{.475\textwidth}
          \centering
          \codecpp{IfThenElse} node definition
          \inputminted[tabsize=2,frame=single,rulecolor=gray,fontsize=\fontsize{4.5}{4}]{cpp}{fig/halide_IfThenElse_node.cpp}
          \vspace{0.4cm}
        \end{subfigure}
      \end{minipage}
    }
  \end{figure}
\end{frame}

\section{Multi-Level Intermediate Representation (MLIR)}

\begin{frame}{\raisebox{-0.25\height}{\includegraphics[height=0.5cm]{img/mlir-identity-09.png}}}
  \begin{itemize}
    \item Multi-Level Intermediate Representation (MLIR): open-source compiler infrastructure project; provides common IR to represent \textbf{multiple levels of abstractions} maintaining a \textbf{unified interface}.
    \item Under LLVM's umbrella.
    \item Address challenges in building compilers and optimizing code generation for modern high-performance computing and machine-learning applications.
          \begin{itemize}
            \item Many compilation and system design problems are better modeled at a \textbf{higher-} or \textbf{lower-level abstraction}. Languages that use LLVM end up developing their IR to solve domain-specific problems. ML frameworks also use domain-specific abstractions (\textquote{ML graphs}).
            \item Makes it \textbf{easy} to define and \textbf{introduce new abstraction levels} and provides the infrastructure to use them to solve common compiler engineering problems.
          \end{itemize}
    \item MLIR infrastructure provides:
          \begin{enumerate}
            \item Standardized Static Single Assignment (\textbf{SSA})-based IR data structures.
            \item Declarative system for defining IR \textbf{dialects}.
            \item Wide range of common infrastructure: documentation, parsing and printing logic, multithreaded compilation support, \textbf{pass management}, etc.
          \end{enumerate}
  \end{itemize}
\end{frame}

\begin{frame}{\raisebox{-0.25\height}{\includegraphics[height=0.5cm]{img/mlir-identity-09.png}} Dialects}
  \begin{itemize}
    \item \textbf{Dialect}: collection of related \textbf{operations}, \textbf{attributes} and \textbf{types} used to represent a particular domain.
          \begin{itemize}
            \item \textit{Attributes}: mechanism for specifying constant data on operations in places where a variable is never allowed (such as the comparison predicate of a \code{cmpi} operation).
          \end{itemize}
    \item MLIR allows for multiple dialects (even those outside of the main code tree) to \textbf{co-exist} together within one \textit{module}. Dialects are produced and consumed by certain \textbf{\textit{passes}}.
    \item Examples:
          \begin{itemize}
            \item \textbf{\textit{Arith}}: arithmetic dialect, holds basic integer and floating point mathematical operations which include: unary, binary, and ternary arithmetic ops, bitwise and shift ops, cast ops, and compare ops. Operations in this dialect also accept vectors and tensors of integers or floats.
            \item \textbf{\textit{Func}}: creation of high-level function abstractions and function calls.
            \item \textbf{\textit{Memref}}: memory reference, provides a collection of operations and types focused on representing and manipulating multi-dimensional arrays, or tensors, in memory.
            \item \textbf{\textit{SCF}}: structured control flow, which includes operations such as loops and conditionals.
            \item \textbf{\textit{Vector}}: supports multi-dimensional vector types and custom operations on them.
            \item \textbf{\textit{Affine}}: affine expressions and affine loops that allows polyhedral model compilation, analysis and optimizations.
          \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{\raisebox{-0.25\height}{\includegraphics[height=0.5cm]{img/mlir-identity-09.png}} IR}
  \begin{itemize}
    \item IR is \textbf{generic} enough to represent ASTs in a language frontend, generated instructions in a target-specific backend, HLS constructs, circuits (CIRCT), etc.
    \item IR is based on a graph-like data structure:
          \begin{itemize}
            \item Nodes: \textbf{\textit{Operations}}
            \item Edges: \textbf{\textit{Values}}
          \end{itemize}
          Each \textit{Value} is the result of exactly one \textit{Operation} or \textbf{\textit{Block Argument}} and has a \textbf{\textit{Value Type}} defined by the type system.
    \item Three forms: human-readable (\code{.mlir}), in-memory and serialized.
  \end{itemize}
  \begin{figure}[H]
    \centering
    \begin{minipage}{0.475\textwidth}
      \inputminted[tabsize=2,frame=single,rulecolor=gray,fontsize=\fontsize{6}{6}]{text}{fig/mlir_ir_example.mlir}
    \end{minipage}
  \end{figure}
\end{frame}

\section{Circuit IR Compilers and Tools (CIRCT)}

\begin{frame}{\raisebox{-0.25\height}{\includegraphics[height=0.5cm]{img/circt-logo.png}} CIRCT}
  \begin{itemize}
    \item \textbf{C}ircuit \textbf{IR} \textbf{C}ompilers and \textbf{T}ools (CIRCT): project built \textbf{on top of MLIR}. Provides a set of tools and libraries to help with the \textbf{design} and verification of \textbf{digital circuits}.
    \item Adds new \textbf{hardware-oriented dialects} such as:
          \begin{itemize}
            \item \textit{\textbf{\code{hw}}}: generic HW dialect where other dialect operations are instantiated.
            \item \textit{\textbf{\code{comb}}}: models digital combinational logic.
            \item \textit{\textbf{\code{seq}}}: models digital sequential logic.
            \item \textit{\textbf{\code{fsm}}}: models finite-state machines.
            \item \textit{\textbf{\code{sv}}}: represents various SystemVerilog-specific constructs.
            \item \textit{\textbf{\code{calyx}}}: represents Calyx IR types and operations.
          \end{itemize}
    \item Provides a static scheduling infrastructure:
          \begin{itemize}
            \item A \textit{problem} is created from the IR (such as \textit{ModuloProblem}).
            \item A \textit{scheduler} solves the problem (list scheduler, LP-based schedulers, etc).
            \item \code{AffineToLoopSchedule} pass uses Calyx operator library for operation latencies and lowers to \code{LoopSchedule} dialect.
          \end{itemize}
    \item \codecpp{circt::createExportVerilogPass()} takes IR and emits SystemVerilog code.
          \begin{itemize}
            \item Style and options controlled by \codecpp{struct circt::LoweringOptions}.
          \end{itemize}
    \item Under LLVM's umbrella.
    \item SiFive contributing to CIRCT (order of magnitude faster than the current Chisel compiler).
  \end{itemize}
\end{frame}

\begin{frame}{\raisebox{-0.25\height}{\includegraphics[height=0.5cm]{img/circt-logo.png}} CIRCT Dialects and conversion passes}
  \begin{figure}[H]
    \includegraphics[width=4.1cm,left]{img/circt-dialectlegend.png}
  \end{figure}
  \begin{figure}[H]
    \centering
    \vspace*{-1.1cm}
    \includegraphics[height=8.1cm]{img/circt-dialects.png}
  \end{figure}
\end{frame}

\section{Calyx}

\begin{frame}[fragile]{\raisebox{-0.25\height}{\includegraphics[height=0.5cm]{img/calyx-logo-text.png}}}
  \begin{itemize}
    \item Many hardware DSLs \textbf{re-engineer} a new \textbf{intermediate language} (IL) and compiler to generate the HW.
    \item \textbf{\textit{Calyx}} is a \textbf{shared IL} along a compiler infrastructure that implements useful optimizations and analyses, so that new hardware DSLs can use it as an \textbf{intermediate step} to quickly generate hardware designs.
    \item In Calyx, \textbf{\textit{components}} correspond to hardware modules (with input and output ports). Each \textit{component} has three distinct sections:
          \begin{itemize}
            \item \textbf{\code{cells}}: the instantion of hardware sub-components that form the \textit{component} being defined.
            \item \textbf{\code{wires}}: set of connection between the sub-components. They can be organized into \textbf{\code{groups}}.
            \item \textbf{\code{control}}: imperative control flow that defines the \textit{component}'s execution schedule (when each \code{group} executes).
          \end{itemize}
          \begin{figure}[H]
            \centering
            \begin{minipage}{0.32\textwidth}
              \inputminted[tabsize=2,frame=single,rulecolor=gray,fontsize=\fontsize{5}{5}]{text}{fig/calyx_component.calyx}
            \end{minipage}
          \end{figure}
    \item Each \code{group} has \textbf{\code{go}} and \textbf{\code{done}} ports. \code{go} triggers execution of the group, \code{done} finishes it. The \code{control} section uses those signals to orchestrate \code{group} execution.
    \item \code{Control} statements: \code{seq}, \code{par}, \code{if} and \code{while}.
    \item An \textit{assignment} can optionally have a \code{guard} expression:
          \begin{figure}[H]
            \centering
            \begin{minipage}{0.275\textwidth}
              \inputminted[tabsize=2,frame=single,rulecolor=gray,fontsize=\fontsize{5}{5}]{text}{fig/guarded_example.calyx}
            \end{minipage}
          \end{figure}
  \end{itemize}
\end{frame}

\section{Methodology}

\begin{frame}{Methodology}
  \begin{itemize}
    \item Converting Halide down to RTL (Xilinx FPGAs bitstream + XRT) is complex and involves \textbf{many steps}.
    \item \textbf{Top-down} and \textbf{incremental and iterative} design methodology. Start from simple Halide pipeline and keep adding complexity (new Halide IR nodes), then add MLIR conversion for them.
          \begin{itemize}
            \item Start with \codecpp{output(x) = x + 42;}, Halide IR nodes: \codecpp{Add}, \codecpp{IntImm}, \codecpp{For} and \codecpp{Store}.
          \end{itemize}
  \end{itemize}
  \begin{figure}
    \centering
    \scalebox{.36}{\input{fig/mlir_to_xilinx.tex}}
  \end{figure}
\end{frame}

\section{Halide MLIR \textit{CodeGen}}

\begin{frame}{Marking loops to be offloaded to an accelerator}
  \begin{enumerate}
    \item \codecpp{InjectAcceleratorOffload} \codecpp{IRMutator} traverses IR and marks \code{For} IR nodes with new \codecpp{enum DeviceAPI::XRT}.
    \item For each marked \code{For} loop:
          \begin{enumerate}
            \item Pass the loop \codecpp{Stmt} and kernel name to virtual class called \codecpp{CodeGen_Accelerator_Dev}
                  \begin{itemize}
                    \item \codecpp{CodeGen_Xilinx_Dev} for \codecpp{enum DeviceAPI::XRT}
                  \end{itemize}
            \item Replace the loop with a call to the Halide runtime that has been implemented for XRT (\code{xrt.cpp}):
                  \begin{itemize}
                    \item \codecpp{halide_xrt_run} to start kernel execution
                    \item But also \codecpp{halide_xrt_initialize_kernels} and \codecpp{halide_xrt_finalize_kernels} before/after the kernel execution to load/unload the kernel (FPGA bitstream).
                  \end{itemize}
          \end{enumerate}
    \item \codecpp{CodeGen_Xilinx_Dev} uses \codecpp{CodeGen_MLIR} internally to generate the high-level MLIR code to be transformed into RTL.
  \end{enumerate}
\end{frame}

\begin{frame}{\code{CodeGen_MLIR}: Halide IR to MLIR conversion: Basics}
  \begin{itemize}
    \item Kernel arguments converted into \code{func}'s \codecpp{FuncOp}:
          \begin{itemize}
            \item Non-buffer arguments: \codecpp{mlir_type_of} converts Halide type to MLIR type.
            \item Buffer arguments: two MLIR arguments are generated:
                  \begin{enumerate}
                    \item 64-bit integer: base offset of the buffer within the assigned AXI interface. Written by the host code prior to kernel execution.
                    \item \codecpp{MemRefType} (\code{Memref} dialect): needed by MLIR to perform load/store accesses. Gets converted into a \textit{Calyx external memory interface} later on. Before accessing the base offset is added. Symbol name has \codecpp{".buffer"} suffix.
                  \end{enumerate}
          \end{itemize}
    \item Subclass of \codecpp{IRVisitor}, \codecpp{CodeGen_MLIR::Visitor}, walks IR tree and emits MLIR.
          \begin{itemize}
            \item Has a \codecpp{void visit(const <NodeType> *)} method for each Halide IR node type
          \end{itemize}
    \item \textquote{Scoped} symbol table maps string $\rightarrow$ \codecpp{mlir::Value}.
          \begin{itemize}
            \item \codecpp{sym_push}, \codecpp{sym_pop}, \codecpp{sym_get}
          \end{itemize}
    \item Helper methods \codecpp{codegen} for \codecpp{Expr} and \codecpp{Stmt}:
          \begin{figure}[H]
            \centering
            \begin{minipage}{0.3\textwidth}
              \inputminted[tabsize=2,frame=single,rulecolor=gray,fontsize=\fontsize{6}{6}]{cpp}{fig/codegen_methods.cpp}
            \end{minipage}
          \end{figure}
          They call \codecpp{accept} method of the \codecpp{Expr}/\codecpp{Stmt} passing \codecpp{this} as the \codecpp{IRVisitor*} argument, which calls the corresponding \codecpp{visit} method of the \codecpp{CodeGen_MLIR::Visitor} class for that \codecpp{Expr} or \codecpp{Stmt} node type.\\
          E.g. \codecpp{codegen(myIntImm)} calls \codecpp{IRVisitor::visit(const IntImm *)}.
  \end{itemize}
\end{frame}

\begin{frame}{\code{CodeGen_MLIR}: Halide IR to MLIR conversion: Basic nodes}
  \begin{itemize}
    \item \codecpp{Let}, \codecpp{LetStmt}: represent the \textquote{let} construct found in many functional programming languages.
          \begin{figure}[H]
            \centering
            \begin{minipage}{0.355\textwidth}
              \inputminted[tabsize=2,frame=single,rulecolor=gray,fontsize=\fontsize{5}{5}]{cpp}{fig/let_conversion.cpp}
            \end{minipage}
          \end{figure}
    \item \codecpp{IntImm}, \codecpp{UIntImm}, \codecpp{FloatImm}: numeric immediates. \codecpp{mlir_type_of} to convert it to the corresponding \codecpp{mlir::Type}.
          \begin{figure}[H]
            \centering
            \begin{minipage}{0.46\textwidth}
              \inputminted[tabsize=2,frame=single,rulecolor=gray,fontsize=\fontsize{5}{5}]{cpp}{fig/immediate_conversion.cpp}
            \end{minipage}
          \end{figure}
    \item \codecpp{Add}, \codecpp{Sub}: addition/subtraction of two \codecpp{Expr}.
          \begin{figure}[H]
            \centering
            \begin{minipage}{0.55\textwidth}
              \inputminted[tabsize=2,frame=single,rulecolor=gray,fontsize=\fontsize{5}{5}]{cpp}{fig/add_conversion.cpp}
            \end{minipage}
          \end{figure}
    \item \codecpp{EQ}, \codecpp{NE}: equality and inequality comparison operations.
          \begin{figure}[H]
            \centering
            \begin{minipage}{0.55\textwidth}
              \inputminted[tabsize=2,frame=single,rulecolor=gray,fontsize=\fontsize{5}{5}]{cpp}{fig/eq_conversion.cpp}
            \end{minipage}
          \end{figure}
  \end{itemize}
\end{frame}

\begin{frame}{\code{CodeGen_MLIR}: Halide IR to MLIR conversion: \code{For} loop}
  \begin{itemize}
    \item \codecpp{For}: the only loop construct that Halide IR has.
          \begin{figure}[H]
            \centering
            \begin{minipage}{0.65\textwidth}
              \inputminted[tabsize=2,frame=single,rulecolor=gray,fontsize=\fontsize{5}{5}]{cpp}{fig/for_conversion.cpp}
            \end{minipage}
          \end{figure}
  \end{itemize}
\end{frame}

\begin{frame}{\code{CodeGen_MLIR}: Halide IR to MLIR conversion: \code{Load}/\code{Store}}
  \begin{itemize}
    \item \codecpp{Load}, \codecpp{Store}: memory accesses.
          \begin{figure}[H]
            \centering
            \begin{minipage}{0.75\textwidth}
              \inputminted[tabsize=2,frame=single,rulecolor=gray,fontsize=\fontsize{5}{5}]{cpp}{fig/load_conversion.cpp}
            \end{minipage}
          \end{figure}
  \end{itemize}
\end{frame}

\section{Lowering MLIR to CIRCT}

\begin{frame}{\code{CodeGen_CIRCT}: Generates generic RTL kernel}
  \begin{itemize}
    \item \codecpp{MemRefType} arguments transformed into \textit{Calyx external memory interface}
    \item New features implemented:
          \begin{itemize}
            \item Passing custom argument names to Calyx
            \item Support for sequential-reads memories (\code{read_en}) and variable memory-access sizes (\code{access_size})
            \item Support more \code{arith} dialect operations such as \codecpp{MinSIOp}
            \item Adding initial vector support (\code{calyx::AssignOp} modified to assign flattened bits $\leftrightarrow$ vectors)
            \item Implement basic \code{vector} dialect operations (such as \codecpp{vector::SplatOp}, a scalar $\rightarrow$ vector broadcast)
          \end{itemize}
  \end{itemize}
  \begin{figure}
    \centering
    \scalebox{.4}{\input{fig/mlir_to_circt.tex}}
  \end{figure}
\end{frame}

\section{Wrapping the generic RTL kernel for Xilinx FPGAs}

\begin{frame}{CIRCT to Xilinx RTL}
  \begin{itemize}
    \item Wraps generic RTL kernel with necessary Xilinx-specific logic: \textit{Calyx external memory} to AXI converters and AXI4-Lite subordinate \textbf{control logic} specified by XRT.
    \item Generates SystemVerilog code and \code{kernel.xml} file needed by Vitis \code{v++}.
  \end{itemize}
  \begin{figure}
    \centering
    \scalebox{.4}{\input{fig/circt_to_xilinx_rtl.tex}}
  \end{figure}
\end{frame}

\begin{frame}{\textit{Calyx external memory} to AXI converter}
  \begin{itemize}
    \item Buffer kernel arguments are accessed through \textit{Calyx external memory interface}.
          \begin{itemize}
            \item Each buffer is mapped into a different interface. This module converts it to AXI.
          \end{itemize}
    \item FSM handles two-way handshake of AXI transfers.
  \end{itemize}
  \begin{figure}[H]
    \begin{minipage}[H]{\textwidth}
      \centering
      \begin{subfigure}[t]{.45\textwidth}
        \centering
        \scalebox{.6}{\input{fig/calyx_ext_mem_to_axi_box.tex}}
        \caption{\textit{Calyx external memory} to AXI converter}
      \end{subfigure}
      \begin{subfigure}[t]{.45\textwidth}
        \centering
        \scalebox{.5}{\input{fig/calyx_ext_mem_to_axi_fsm.tex}}
        \caption{\textit{Calyx external memory} to AXI converter FSM}
      \end{subfigure}
    \end{minipage}
  \end{figure}
\end{frame}

\begin{frame}{Kernel control logic}
  \begin{itemize}
    \item Support seamless integration with Xilinx runtime libraries: XRT-managed kernel control requirements.
    \item Kernel acts as an AXI4-Lite subordinate and exposes a set of registers that are used by the host to control the kernel execution and set up the kernel.
          \begin{itemize}
            \item Control register with \code{start} and \code{done} bits.
            \item Interrupt registers to enable/mask interrupts.
            \item At offset \codecpp{+0x10} kernel arguments.
          \end{itemize}
    \item Interrupt line signals events kernel $\rightarrow$ host to avoid continuous polling.
  \end{itemize}
  \begin{figure}[H]
    \begin{minipage}[H]{\textwidth}
      \centering
      \begin{subfigure}[t]{.45\textwidth}
        \centering
        \scalebox{.5}{\input{fig/control_if_read_handler.tex}}
        \caption{Control interface read handler FSM.}
      \end{subfigure}
      \begin{subfigure}[t]{.45\textwidth}
        \centering
        \scalebox{.5}{\input{fig/control_if_write_handler.tex}}
        \caption{Control interface write handler FSM.}
      \end{subfigure}
    \end{minipage}
  \end{figure}
\end{frame}

\section{Halide runtime XRT backend}

\begin{frame}{Halide runtime XRT backend (\code{xrt.cpp})}
  \begin{itemize}
    \item Implements \codecpp{struct halide_device_interface_impl_t} interface:
          \begin{itemize}
            \item \codecpp{halide_xrt_device_malloc}: allocate buffers that the device can access.
            \item \codecpp{halide_xrt_device_free}: deallocate them.
            \item \codecpp{halide_xrt_copy_to_device}: copy a buffer to device/flush CPU cache so that all changes are visible to the device.
            \item \code{halide_xrt_copy_to_host}: copy buffer to host/flush all the relevant device buffers and invalidate CPU cache so that all changes are visible to the host.
          \end{itemize}
    \item Also implements functions called directly when \code{For} loop is offloaded to the accelerator:
          \begin{itemize}
            \item \codecpp{halide_xrt_initialize_kernels}: loads a kernel into the device.
            \item \codecpp{halide_xrt_finalize_kernels}: unload the kernel.
            \item \codecpp{halide_xrt_run}: starts kernel execution with the specified arguments.
          \end{itemize}
    \item Allocating buffers depends on loading kernel first, so they are \textbf{lazily allocated}: just prior to kernel execution.
    \item Uses XRT's C API. Seamless integration with Halide.
  \end{itemize}
\end{frame}

\section{Bugs and issues}

\begin{frame}[fragile]{Bugs and issues}
  \begin{itemize}
    \item MLIR and CIRCT are still experimental projects.
    \item MLIR SCF $\rightarrow$ Calyx pass and Calyx $\rightarrow$ HW dialects had \textbf{never been tested on real hardware} (or even cycle-accurate simulator)
    \item Numerous bugs were found. Waveform debugging was essential.
    \item Bugs were fixed, submitted to upstream, and already merged:
          \begin{itemize}
            \item Add support for multiple \codecpp{calyx::AssignOp} with guards to the same destination
                  \begin{figure}
                    \scalebox{.5}{\input{fig/multiple_guards_assign.tex}}
                  \end{figure}
            \item Clock-enable with the \code{done} signal when writing to Calyx registers
                  \begin{figure}[H]
                    \centering
                    \includegraphics[width=8cm,valign=b]{img/bug_write_en_done.png}
                    $\implies$
                    \adjustbox{valign=c}{\scalebox{.5}{\input{fig/write_en_done_fix.tex}}}
                  \end{figure}
            \item \codecpp{calyx::NotLibOp} was lowered incorrectly (\textit{XOR} with 0s instead of 1s)
            \item Read/write-enable signals of external memories were left unconnected
          \end{itemize}
  \end{itemize}
\end{frame}

\section{Evaluation}

\begin{frame}{Evaluation}
  \begin{itemize}
    \item Avnet Ultra96-V2 Board with 2 GB LPDDR4:
          \begin{table}[H]
            \centering
            \scalebox{.5}{\input{fig/table_ps.tex}}
            \scalebox{.5}{\input{fig/table_pl.tex}}
          \end{table}
    \item 3 Halide kernels used to evaluate the generated RTL code
    \item For each kernel, resource utilization and execution time analysis performed
    \item Each kernel has been run three times and the average time has been taken
    \item The result (output data) of RTL execution binary-compared with CPU execution (\textquote{golden model})
          \begin{itemize}
            \item Timing of CPU execution taken with vanilla kernel without Halide scheduling directives (defaults to \textit{inline}/\textit{total fusion})
          \end{itemize}
    \item For all the kernels, the ${\SI{150}{\mega\hertz}}$ target frequency was met.
          \begin{itemize}
            \item Note: ${\SI{150}{\mega\hertz}}$ on the FPGA, while the CPU runs at ${\SI{1.5}{\giga\hertz}}$, 10 times faster
          \end{itemize}
  \end{itemize}

\end{frame}

\begin{frame}{\textit{Test load} kernel}
  \begin{itemize}
    \item Tests the load and store from external memory functionality (32-bit ints):
          \begin{figure}[H]
            \centering
            \begin{minipage}{0.3\textwidth}
              \inputminted[tabsize=2,frame=single,rulecolor=gray,fontsize=\fontsize{5}{5}]{cpp}{fig/test_load_kernel.cpp}
            \end{minipage}
          \end{figure}
    \item Evaluation: vect. factors: powers of two from 1 to 32. AXI data bus widths: vector size and max (1024).
    \item Resource usage:
          \begin{table}[Ht]
            \makebox[\linewidth]{
              \scalebox{.333}{\input{fig/test_load_clb_usage.tex}}
              \scalebox{.333}{\input{fig/test_load_bram_usage.tex}}
              \scalebox{.333}{\input{fig/test_load_arith_usage.tex}}
            }
          \end{table}
          \begin{itemize}
            \item \textbf{1024-bit AXI} leads to a \textbf{considerable increase in CLB LUT as Logic}: extra logic needed to drive the data bus wires in the \textit{Calyx external memory to AXI converter}.
            \item Native vector AXI width size: $39.29\%$ up to $58.21\%$ and $28.81\%$ up to $40.78\%$, for vectorization factors $1$ and $32$, respectively.
          \end{itemize}
    \item Execution time: buffer with $4 \times 1024 \times 1024$ 32-bit ints (\qty{16}{\mega\byte} in total).
          \begin{table}[H]
            \centering
            \scalebox{.333}{\input{fig/test_load_exec_time.tex}}
          \end{table}
  \end{itemize}
\end{frame}

\begin{frame}{\textit{Test load div int8} kernel}
  \begin{itemize}
    \item Take advantage of vectorization support by using 8-bit integers:
          \begin{figure}[H]
            \centering
            \begin{minipage}{0.3\textwidth}
              \inputminted[tabsize=2,frame=single,rulecolor=gray,fontsize=\fontsize{5}{5}]{cpp}{fig/test_load_div_int8.cpp}
            \end{minipage}
          \end{figure}
    \item Evaluation: vect. factors powers of two from 4 to 128, and native and 1024-bit AXI data bus width.
    \item Resource usage:
          \begin{table}[Ht]
            \makebox[\linewidth]{
              \scalebox{.333}{\input{fig/test_load_div_int8_clb_usage.tex}}
              \scalebox{.333}{\input{fig/test_load_div_int8_bram_usage.tex}}
              \scalebox{.333}{\input{fig/test_load_div_int8_arith_usage.tex}}
            }
          \end{table}
          \begin{itemize}
            \item Same as before: increase in the logic when native vector AXI vs 1024-bits.
            \item Division by constant, the synthesizer used logic and did \textbf{not use DSPs} to implement it.
          \end{itemize}
    \item Execution time: buffer with $32 \times 1024 \times 1024$ 8-bit ints.
          \begin{table}[H]
            \centering
            \scalebox{.333}{\input{fig/test_load_div_int8_exec_time.tex}}
          \end{table}
          \begin{itemize}
            \item After vect. factor of 32, it runs faster than the CPU
            \item With vector. factor of 128 elements (1024-bit AXI accesses), it is \textbf{$\times 2.6$ faster}.
          \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{\textit{Test blur3x3 sliding window} kernel (1)}
  \begin{itemize}
    \item 3x3 blur filter on a 2D array of 32-bit ints. Tries to exploit most of the FPGA characteristics and supported features implemented.
          \begin{figure}[H]
            \centering
            \begin{minipage}{0.5\textwidth}
              \inputminted[tabsize=2,frame=single,rulecolor=gray,fontsize=\fontsize{5}{5}]{cpp}{fig/test_blur3x3_sliding_window.cpp}
            \end{minipage}
          \end{figure}
    \item Exploit device locality and loads from external memory by using \textbf{sliding windows within tiling} with device-local buffers.
    \item Evaluation: since \textbf{vector accesses to local memories were not implemented for this thesis}, \textbf{vectorization can not be used}. Different \textbf{tile sizes} will be evaluated: $8 \times 8$, $16 \times 16$ and $32 \times 32$.
          \begin{itemize}
            \item AXI data bus width was set to the element size (32 bits)
          \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{\textit{Test blur3x3 sliding window} kernel (2)}
  \begin{itemize}
    \item Resource usage:
          \begin{table}[Ht]
            \makebox[\linewidth]{
              \scalebox{.4}{\input{fig/test_blur3x3_sliding_window_clb_logic_usage.tex}}
              \scalebox{.4}{\input{fig/test_blur3x3_sliding_window_local_mem_requirements.tex}}
              \scalebox{.4}{\input{fig/test_blur3x3_sliding_window_bram_usage.tex}}
              \scalebox{.4}{\input{fig/test_blur3x3_sliding_window_bram_primitives_usage.tex}}
              \scalebox{.4}{\input{fig/test_blur3x3_sliding_window_arith_usage.tex}}
            }
          \end{table}
          \begin{itemize}
            \item CLB utilization barely changes: from control PoV the only change are loop boundaries (constants).
            \item Sliding buffers allocated as distributed memories in the CLBs (as SRAMs).
            \item Multiplication of $3\times 3 = 9$ input elements for each output using DSPs. Two divs constant $3$ using logic as before.
          \end{itemize}
    \item Execution time: buffer with $4096 \times 4096$ 32-bit ints.
          \begin{table}[H]
            \centering
            \scalebox{.333}{\input{fig/test_blur3x3_sliding_window_exec_time.tex}}
          \end{table}
          \begin{itemize}
            \item \textbf{Local buffers and vectorization is currently unsupported}: kernel code not vectorized and accesses to external memory limited \textbf{32 bits}. Execution on the FPGA 10 times slower than on the CPU.
            \item Execution time reduces when increasing the tile size: thanks to the \textbf{reduction of re-computation of tile boundaries}.
          \end{itemize}
  \end{itemize}
\end{frame}

\section{Conclusions and future work}

\begin{frame}{Conclusions}
  \begin{itemize}
    \item FPGAs are \textbf{highly flexible devices}. Enable the power of reconfigurable hardware for a wide range of applications.
    \item However, developing FPGA applications is a \textbf{complex task}: requires a deep understanding of both hardware and software design flows.
    \item \textbf{DSLs} have emerged to help simplify the development process by providing \textbf{higher levels of abstraction} and focusing on specific problem domains.
    \item \textbf{Halide} is a popular DSL designed for expressing \textbf{image and array processing} and computational photography algorithms.
    \item In this thesis, a \textbf{new backend} for Halide which \textbf{targets FPGAs} has been developed.
          \begin{itemize}
            \item \textbf{Generic RTL kernel} is generated. Then wrapped for Xilinx FPGA devices.
          \end{itemize}
    \item Instead of directly generating RTL or HLS code, \textbf{generic MLIR} is first generated.
          \begin{itemize}
            \item \textbf{Can target many devices and acceleration APIs}
          \end{itemize}
    \item \textbf{Novel flow} using \textbf{CIRCT} and \textbf{Calyx} to convert generic MLIR has been implemented.
          \begin{itemize}
            \item Leverages the flexibility and extensibility of MLIR and CIRCT
          \end{itemize}
    \item Results presented in this thesis show that the approach is \textbf{viable} and can be used to generate efficient FPGA code from Halide programs.
          \begin{itemize}
            \item Still \textbf{work to do} and features to be added to improve the generated code.
          \end{itemize}
    \item Contributed to open source projects by finding bugs, fixing them, and adding and bringing the necessity of new features.
  \end{itemize}
\end{frame}

\begin{frame}{Future work}
  \begin{itemize}
    \item Support vectorized accesses to local memory
    \item Improved support for MLIR's \code{arith} \textit{min} and \textit{max} operations
    \item Generalize \codecpp{MemRefType} lowering
    \item Proper support for \codecpp{scf::IfOp} in CIRCT's \code{SCFToCalyx} pass
    \item Implement lowering of \codecpp{calyx::ParOp} in \code{CalyxToHW}
    \item Add floating-point support in the MLIR to RTL lowering
    \item Avoid useless pipeline stages after \code{comb} canonicalization of lowered pipelined Calyx operations
    \item Emit loops and memory accesses using MLIR's \code{affine} dialect
    \item Use CIRCT's static scheduling infrastructure to lower MLIR to Calyx
    \item Add AXI-Stream support
    \item Coalescing buffer to implement write-combining
    \item Experiment with HLS code generation from MLIR
    \item Halide autoschedulers for FPGA targets
    \item $\ldots$
  \end{itemize}
\end{frame}

\begin{frame}{}
  \centering \Large
  \emph{Thank you for your attention.}
\end{frame}

\end{document}
